# Агрегатор CRUD действий пользователей (Рекомендуемый функционал)

Этот проект представляет собой систему для пакетной обработки логов действий пользователей и вычисления агрегированных показателей на основе данных за последние 7 дней. Система использует Apache Spark для параллельной обработки больших объемов данных и агрегирования по типам действий (CREATE, READ, UPDATE, DELETE).

Процесс выполняется ежедневно в 7 утра, подсчитывая количество каждого действия для каждого пользователя за предыдущие 7 дней. Результат записывается в CSV файл.



### Формат входных данных

Каждый CSV файл в директории `input` представляет собой логи действий пользователей за один день. 

Для генерации логов можно воспользоваться скриптом `generate.py`.
```bash
python3 generate.py <dir to put files> <%Y-%m-%d date> <days_count> <unique emails count> <events count>
```
Например:
```bash
python3 generate.py input 2024-09-10 30 10 2000
```

**Формат входного файла:**
- **email** — email пользователя
- **action** — тип действия (CREATE, READ, UPDATE, DELETE)
- **dt** — дата и время выполнения действия в формате `YYYY-mm-dd HH:MM:SS`

**Пример входных данных в CSV:**
```
BYWDBz@ya.ru,DELETE,2024-09-10 16:01:32
RssCui@mail.ru,CREATE,2024-09-10 00:03:34
BYWDBz@ya.ru,DELETE,2024-09-10 19:49:59
GTeZH@mail.ru,UPDATE,2024-09-10 05:23:02
kHpHM@ya.ru,READ,2024-09-10 20:37:38
```

### Формат выходных данных

Файл с результатом агрегирования создается в директории `output` с именем по шаблону `YYYY-mm-dd.csv` (например, `2024-09-16.csv`). Он содержит суммарное количество CRUD действий каждого пользователя за последние 7 дней до указанной даты. То есть файл `2024-09-16.csv` содержит агрегат действий по пользователям от 2024-09-09 до 2024-09-15 включительно.

**Структура выходного CSV файла:** 
- **email** — почта пользователя,
- **create_count** — количество операций CREATE,
- **read_count** — количество операций READ,
- **update_count** — количество операций UPDATE,
- **delete_count** — количество операций DELETE.

**Пример выходных данных в CSV:**
```
email,create_count,read_count,update_count,delete_count
AjdIjF@mail.ru,362,322,370,344
BYWDBz@ya.ru,348,340,346,365
FEXVfhXnUNAV@mail.ru,369,370,346,326
```


## Использование

Для запуска проекта вам потребуется установленный `Docker` и `Docker Compose`.



### Шаги для установки:

1. Склонируйте репозиторий или скачайте проект:

    ```bash
    git clone --branch recommended https://github.com/lampedusa238/crud-aggregator.git
    ```

2. Создайте файл `.env` с UID текущего пользователя:

    ```bash
    echo -e "AIRFLOW_UID=$(id -u)" > .env
    ```
    Это позволяет Airflow запускаться от имени текущего пользователя и избегать проблем с доступом к файлам и правам в контейнере.\

3. Запустите Docker Compose для развертывания Airflow и Spark в контейнерах:
    ```bash
    docker-compose up -d
    ```
4. Дождитесь развертывания контейнеров и убедитесь в их успешном запуске перейдя по адресам: 
    - http://localhost:8082 — Spark
    - http://localhost:8080 — Airflow
5. Авторизуйтесь в Airflow по адресу http://localhost:8080 указав данные:
    - login: `airflow`
    - password: `airflow`
6. Проверьте, что в списке дагов(DAG) присутсвует `aggregate_logs`, который запускается каждый день в 7 утра по **UTC** (**Schedule**: `0 7 * * *`), для ежедневного запуска в 7 утра по **MSK** следует указать время на 3 часа раньше, т.е. 4 утра по **UTC** (**Schedule**:`0 4 * * *`).


Для запуска Spark-скрипта в Docker-контейнере Spark используется `BashOperator` c командой:
```bash
docker exec crud-aggregator-spark-master-1 \
    /opt/bitnami/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    /scripts/spark_script.py
```
где:
- `docker exec crud-aggregator-spark-master-1` отвечает за выполнение команды внутри контейнера crud-aggregator-spark-master-1.
- `/opt/bitnami/spark/bin/spark-submit` используется для запуска Spark приложения
- `--master spark://spark-master:7077` указывает на мастер-ноду кластера Spark (по адресу spark-master:7077).
- `--deploy-mode client` выбирает режим развертывания "клиент", в котором драйвер программы запускается на клиенте
- `/scripts/spark_script.py` является путем к Python-скрипту, который выполняет агрегацию логов в Spark.


В результате выполнения скрипта `spark_script.py` считываются данные за 7 дней до текущей даты из `/input`, данные CRUD-операций агрегируются по пользователям (e-mail), а результат записывается в папку `/output`.
